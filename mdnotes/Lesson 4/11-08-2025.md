# CNN TensorFlow implementation

Adding 'Dropout Regularization' - more reading

Added a MaxPooling layer

Example illustrates deeper networks can perform better

***

# AlexNet

First came to prominence after demonstrating efficacy in ImageNet Top-5 error test

CNN for colour images

Network is split in two parallel networks to run on two separate GPUs

Many cascaded conv layers plus max-pooling, followed by a couple of fully-connected layers.

8 total layers.


***

# VGGNet

***
Aside:

Wondering - do you ever get something like 'channel degeneracy' in conv layers?
***

Demonstrated improvements in accuracy over AlexNet by deepening the network

***

# GoogLeNet

'Inception' Network/Architecture

Actually named after the Chris Nolan film due to the line 'We need to go deeper'.

Concatenates channels from layers run in parallel

***

# ResNet

Insight was that deeper networks become harder to train.

Introduces 'skip connections'

To do with 'learning the identity function'

***

Example given where pre-trained ResNet parameters can be loaded

***

# "Amplify your data"

Idea is to use a pre-trained starting model trained on a general dataset and then continuing training with a specific dataset

"Data Augmentation" - take a single training example and cut up/rotate/modify brightness of the image (or do equivalent things in other data modes) and add those modified samples back to the training set

'Transfer learning' - missed this a bit. More reading needed

***

'Depth-wise Separable Convolutions' - enables output channel weight sharing. Didn't quite get this

'EfficientNet' - find the optimal combination of network depth, layer resolution and channel count.