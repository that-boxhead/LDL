# Reccurent Neural Networks and Time Series

Nonsequential vs sequential crossed with regression vs classification

Sequential does not mean timed neccesarily.

***

15/08/2025

After further reading:

Could have a feedforward dense layer after a recurrent layer with all the hidden recurrent neurons feeding forward.

***

Training data for a single example consists of a vector of values throughout some interval of time.

Minibatching - becomes neccesary to zero-pad examples in time so they become the same length.

***

RNNs are especially sensitive to vanishing gradient

With the time-direction parts of backpropagation, without mitigation, you either get gradient blowup or gradient vanishing.

"Skip connections" are one example of a mitigation

***

LSTM cell - at each timestep, you have gates (made of sigmoid neurons) which decide whether to admit the previous state to the current activation.

The gating behaviour itself is learned

LSTM cells replace original hidden layer neurons. Outputs feed back in the same way

Not quite understanding 'Constant Error Carousel'

***

Classification/next word/next letter prediction

Uses autoregression - feed network output back as an input for next prediction

Can also use 'beam search' - take top n candidates for next token and perform autoregression on all n candidates.

End up branching to $n^m$ predictions, where $m$ is the number of predicted tokens