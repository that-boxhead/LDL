# Perceptron

Weighted sum of a vector of inputs.

Output passes through a "sign" function.

Returns +1 if the sum is positive, -1 if negative.

Perceptron has a bias input of contant value 1. This also has a learned weight.

***

Perceptron output $y$ is given as

$$
y = f(z)
$$

$$
z = \sum_{i=0}^n w_i x_i
$$

For the sign function:

$$

f(z) = \begin{cases}

-1 & z < 0 \\
1 & z \geq 0

\end{cases}

$$

and the bias weight:

$x_0 = 1$

***

Perceptron training based on output sensitivity to weight changes.

Require a "ground truth" - supervised learning.

***

### v2_2 Programming example

Performing training on all training examples once is called an "epoch".

See the learning rate in the v2_2 notebook.

Weights are nudged in the right direction by a fixed amount for each incorrect result.

***

### Bias Term

In effect, the bias weight moves the sign function transition point.

$$
z = w_0 + \sum_{i=1}^n w_i x_i = w_0 + z_{1..n}
$$

$$

f(z) = \begin{cases}

-1 & z_{1..n} < -w_0 \\
1 &  z_{1..n} \geq -w_0

\end{cases}

$$

***

## Linear algebra for Neural Nets

Features - $\mathbf{x}$

Weights - $\mathbf{w}$

Then $z=\mathbf{x}^\text{T}\mathbf{w}=\mathbf{x}\cdot\mathbf{w}$

***

Rabbit hole:

Inner product of real vectors is equivalent to matrix product

$\mathbf{x}^\text{T}\mathbf{y}$

Outer product of real vectors is equivalent to matrix product

$\mathbf{x}\mathbf{y}^\text{T}$

Outer product also generally called the __tensor product__ for things other than simple vectors.

- [Outer product - Wikipedia](https://en.wikipedia.org/wiki/Outer_product)
- [Tensor product - Wikipedia](https://en.wikipedia.org/wiki/Tensor_product)
- [Tensor algebra - Wikipedia](https://en.wikipedia.org/wiki/Tensor_algebra)
- [Free algebra - Wikipedia](https://en.wikipedia.org/wiki/Free_algebra)

***

$y = \text{sign}(\mathbf{x}\cdot\mathbf{w})$

For multiple perceptrons in parallel, weights become a matrix of row vectors

$\mathbf{w}=\begin{bmatrix}\mathbf{w_0} \\ \mathbf{w_1} \\ \vdots \end{bmatrix}$

Multiple perceptrons/neurons with one set of features (? hope usage is correct. Being referred to as 'inputs') becomes matrix-vector multiplication

Multiple neurons each with separate features becomes matrix-matrix multiplication.

***

## Perceptron Limitations

Individual neuron with some number of inputs has a "decision boundary".

This is graphable as a straight line for a 2-input perceptron.

For the NAND example, the classes are successfully separable by a decision boundary that is a straight line.

NAND classes are "Linearly Separable"

XOR is not linearly separable, so the perceptron will never converge.

But a network of perceptrons will converge.

Teacher suggests thinking about creating logic from combinations of NORs/NANDs.

***

## Gradient Descent

Derivative of y w.r.t. x is analogous to the sensitivity of y to x.

Idea is to minimise the error between model prediction and ground truth.

MSE is a way of measuring total error. MSE represents a function over the input/feature space.

Find the global minimum of this function.

Gradient Descent is a way of doing this.

$x_{n+1}=x_n - \eta f'(x_n)$

Start with an initial guess.

Then the next guess is the previous guess minus the gradient of the error function at your guess, scaled by the learning factor. Repeat.

$\eta$ is the learning rate

