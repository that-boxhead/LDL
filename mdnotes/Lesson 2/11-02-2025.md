# Perceptron

Weighted sum of a vector of inputs.

Output passes through a "sign" function.

Returns +1 if the sum is positive, -1 if negative.

Perceptron has a bias input of contant value 1. This also has a learned weight.

***

Perceptron output $y$ is given as

$$
y = f(z)
$$

$$
z = \sum_{i=0}^n w_i x_i
$$

For the sign function:

$$

f(z) = \begin{cases}

-1 & z < 0 \\
1 & z \geq 0

\end{cases}

$$

and the bias weight:

$x_0 = 1$

***

Perceptron training based on output sensitivity to weight changes.

Require a "ground truth" - supervised learning.

***

### v2_2 Programming example

Performing training on all training examples once is called an "epoch".

See the learning rate in the v2_2 notebook.

Weights are nudged in the right direction by a fixed amount for each incorrect result.

***

### Bias Term

In effect, the bias weight moves the sign function transition point.

$$
z = w_0 + \sum_{i=1}^n w_i x_i = w_0 + z_{1..n}
$$

$$

f(z) = \begin{cases}

-1 & z_{1..n} < -w_0 \\
1 &  z_{1..n} \geq -w_0

\end{cases}

$$