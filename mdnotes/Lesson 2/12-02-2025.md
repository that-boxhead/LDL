## Chain Rule Computation

"Differentiable Activation Functions"

Tanh and Logistic

S-shaped

***

See Black 'n Red notebook on Two-Neuron Error Function Gradient

For weight training:

$$

\mathbf{w}_{n+1} = \mathbf{w}_{n} - \mathbf{\eta} \odot \nabla \text{error}

$$

$\nabla \text{error}$ is composed of $\frac{\partial\text{error}}{\partial w_n}$ for the $n$ weights used in the network.

Don't see a reason why you couldn't have different learning rates for different weights?

$\odot$ denotes the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (elementwise multiplication)