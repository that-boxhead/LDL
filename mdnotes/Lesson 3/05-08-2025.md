05/08/2025

Neuron saturation

Inputs far away from the threshold of the activation function will have little to no effect on weight adjustment as the activation functions have low derivatives there.

Ideally you want inputs close to the threshold.

***

Vanishing Gradients

Activation functions with gradients at the threshold lower than 1 will have a vanishing effect on weight adjustment if cascaded in multiple layers.

Hence tanh is preferred in hidden layers so adjustments don't vanish.

***

Standardize data to avoid saturation at the input layer

'Batch Normalization' - does something similar between layers rather than just at the input layer.

***


Weight initialization can help to avoid saturation

Effect of extreme values of weighted sums - large number of inputs can mean large extreme values

'Glorot' and 'He' normalization


***


Cross-entropy loss works better than MSE as an error/loss function

Grows very fast when further away from the desired output, so gives you better weight adjustment even when hidden layer derivatives are low.

***

Lots of other neuron functions. Some get rid of neuron saturation altogether.